Разлика между Supervised, Unsupervised и Reinforcement Learning
Тип ML	Описание	Примери
Supervised Learning	Моделът се обучава на основа на данни с етикети (input-output). Целта е да се предвиди изходна стойност за нови данни.	
- Кредитен риск (предсказване на "default" или "non-default").
- Прогнозиране на отлив.
Unsupervised Learning	Моделът намира скрити структури в данните, които нямат етикети (само input).	
- Групиране на клиенти (segmentation).
- Намаляване на размерността (PCA).
Reinforcement Learning	Моделът се обучава чрез взаимодействие със средата и получава награди/наказания за своите действия.	
- Оптимизация на кредитно портфолио.
- Търговски стратегии.


#############################################
Алгоритми за Supervised Learning (с надзор)

##### Регресия

1.Линейна регресия (Linear Regression):
Използва се за прогнозиране на непрекъснати стойности (например цена на жилище).

Какво представлява хипотезата в линейната регресия?
Хипотезата е линейна функция, която моделира връзката между входните променливи (features) и изходната променлива (target):

Как се оценява грешката (MSE/MAE)?
MSE (Mean Squared Error): Средната стойност на квадрата на грешките:
MAE (Mean Absolute Error): Средната абсолютна стойност на грешките:

Как се интерпретират коефициентите?
Коефициентите показват как промяната на съответната характеристика с единица влияе на целевата променлива, при фиксиране на останалите характеристики.



2.Логистична регресия (Logistic Regression):
Използва се за класификация (например бинарна класификация - одобрение на кредит).


Каква е функцията, използвана за класификация?
Логистичната регресия използва сигмоидна функция за прогнозиране на вероятности

Как интерпретираме изхода?
Изходът е вероятност за принадлежност към даден клас (обикновено бинарен). Ако вероятността е над 0.5, се приема клас 1, иначе клас 0.


3. Полиномиална регресия (Polynomial Regression)
Полиномната регресия е статистически модел, който представлява разширение на линейната регресия, но с използване на полиномни (степенни) функции вместо само линейна зависимост.
Кога се използва полиномиална регресия?
✅ Когато зависимостта между променливите не е линейна
✅ При прогнозиране на сложни тенденции
✅ В машинното обучение и статистиката за анализ на нелинейни зависимости


##### Класификация

1.Decision Trees (Решаващи дървета)

Подходящи за разбираеми и лесно интерпретируеми модели.

Какво представляват критериите за разделяне?
Gini Index: Мярка за чистота на клъстерите. По-малка стойност = по-хомогенен клъстер.
Entropy (Information Gain): Мярка за намаляване на несигурността при разделяне.

Какви са предимствата и недостатъците?
Предимства: Лесни за разбиране и интерпретация, подходящи за категориални и числови данни.
Недостатъци: Могат да overfit-нат, ако са твърде дълбоки.

2.Random Forest (Случайни гори)
Съставен алгоритъм, който комбинира множество решаващи дървета.

Как намалява overfitting?
Комбинира множество независими дървета и усреднява резултатите, което намалява пренасищането с данни.

Как се определя значимостта на характеристиките?
Значимостта се изчислява въз основа на това колко пъти и колко ефективно дадена характеристика участва в разделянето на данните.

Random Forest е мощен алгоритъм за машинно обучение, който използва концепцията за ансамбли. Основните характеристики на Random Forest включват:
1. Ансамблов метод
Random Forest комбинира множество решаващи дървета (decision trees) в един ансамблов модел.
Крайният резултат се базира на:
Средна стойност на предвижданията за регресионни задачи.
Гласуване с мнозинство за класификационни задачи.
2. Случайност (Randomness)
Случайност в избора на данни:
Използва техниката Bagging (Bootstrap Aggregation), като всяко дърво се обучава на случайно избран поднабор от данните.
Случайност в избора на характеристики (features):
При всяко разделяне на възел в дървото, Random Forest избира само поднабор от характеристиките вместо всички.
3. Редуциране на Overfitting
Чрез комбиниране на много дървета и използване на случайност в обучението, Random Forest намалява риска от overfitting спрямо единичните решаващи дървета.
4. Нелинейни зависимости
Random Forest може да улавя сложни и нелинейни зависимости между характеристиките, тъй като дърветата разделят пространството на данните на сложни области.
5. Оценка на важността на характеристиките
Random Forest може да изчисли важността на характеристиките, като измерва колко често дадена характеристика се използва за разделяне на възел в дърветата.
6. Вътрешна валидирация с OOB (Out-of-Bag Error)
Моделът използва данни, които не са били избрани в случайния поднабор (out-of-bag data), за да оцени точността си, без да е нужно отделно тестово множество.
7. Устойчивост към шум и липсващи данни
Random Forest е устойчив на:
Шум в данните, защото ансамбловият подход заглушава влиянието на грешните предвиждания.
Липсващи стойности, тъй като разделянията в дърветата не зависят задължително от всички характеристики.
8. Поддръжка на висока дименсионалност
Random Forest работи добре с голям брой характеристики, като избира поднабори от тях за всяко дърво.
Random Forest може да се използва за:
Класификационни задачи (например предвиждане на класове).
Регресионни задачи (например предвиждане на числени стойности).
10. Контролируеми хиперпараметри
Random Forest има няколко важни хиперпараметри, които позволяват настройка на модела според задачата:
n_estimators: Броят на дърветата в гората.
max_depth: Максималната дълбочина на всяко дърво.
max_features: Максималният брой характеристики, които да се използват за разделяне.
min_samples_split: Минималният брой примери, необходими за разделяне на възел.
min_samples_leaf: Минималният брой примери в листо.
Недостатъци на Random Forest
По-голямо време за обучение спрямо единични дървета.
Трудна интерпретация на модела, тъй като комбинира множество дървета.
Неефективен при екстремно големи данни, когато няма достатъчно ресурси.


3.SVM (Support Vector Machines)
Support Vector Machines е мощен и популярен алгоритъм за класификация и регресия, който работи като намира оптималната хиперплоскост, която разделя данните на различни класове с максимален марж.

Основни концепции
Хиперплоскост (Hyperplane):
Хиперплоскостта е границата, която разделя данните на два или повече класа. За двумерно пространство тя е права линия, за тримерно е равнина, а за по-високи размерности е хиперплоскост.

Марж (Margin):
Това е разстоянието между хиперплоскостта и най-близките точки от всеки клас. Целта на SVM е да максимизира този марж, за да осигури най-добрата разделителна способност.

Support Vectors:
Това са точките, които са най-близо до хиперплоскостта и определят нейното положение. Те са критични за модела, тъй като ако бъдат премахнати, хиперплоскостта ще се промени.

Как работи SVM?
Линеен SVM:
Алгоритъмът намира праволинейна хиперплоскост, която разделя данните.
Подходящ за линейно разделими данни.
Нелинеен SVM:
Когато данните не могат да се разделят линейно, се използва kernel trick.
Kernel trick трансформира данните в пространство с по-висока размерност, където те стават линейно разделими.

Kernel Trick
Kernel Trick е техника, използвана в алгоритми като Support Vector Machines (SVM), за да се справят с нелинейни зависимости в данните. Основната идея е:
Прехвърляне в пространство с по-висока размерност: Вместо да работим в оригиналното (често линейно) пространство на характеристиките, Kernel Trick позволява данните да бъдат преобразувани в пространство с по-висока размерност, където могат да бъдат по-лесно разделени.
Без директно изчисляване: Вместо експлицитно да се изчисляват координатите на новото пространство, Kernel Trick изчислява само скаларните произведения между точките в това пространство. Това значително спестява време и ресурси.
Пример:
Оригиналните данни в 2D пространство може да са нелинейно разделими.
Kernel Trick ги прехвърля в 3D пространство, където стават линейно разделими.
Популярни ядра (kernels):
Linear Kernel: Работи без прехвърляне на данните в по-висока размерност.
Polynomial Kernel: Използва полиномиална трансформация на данните.
Radial Basis Function (RBF): Моделира сложни нелинейни зависимости.
Sigmoid Kernel: Прилича на сигмоидната функция, използвана в невронните мрежи.

Значение на параметрите 𝐶 и 𝛾 (gamma)
C (Regularization Parameter)
Контролира разменната стойност между:
Максимизиране на маржовете (margins): Стремежът да се създаде модел, който разделя класовете възможно най-добре.
Минимизиране на грешките (classification error): Позволява на модела да игнорира шум в данните.
Как влияе:
Голямо 𝐶: Стреми се да класифицира всички точки правилно, но може да доведе до overfitting.
Малко 𝐶: Позволява повече грешки, което прави модела по-обобщаващ (underfitting).
𝛾 (Gamma) в RBF и други нелинейни ядра
Определя влиянието на отделните точки:
Как всяка точка от обучаващите данни влияе върху модела.
Как влияе:
Голямо 𝛾: Всяка точка има голямо влияние, което може да направи модела много сложен и чувствителен към шум (overfitting).
Малко 𝛾: Всяка точка влияе върху по-голяма област, което може да доведе до по-груб модел (underfitting).

Пример с интуиция
Представи си задача за разделяне на два класа (сини и червени точки):
𝐶: Контролира дали искаме да избегнем грешки (строга граница) или сме готови да толерираме грешки (по-гъвкава граница).
𝛾: Определя "обхвата" на влияние на всяка точка върху хиперплоскостта. Голямо 
𝛾 означава, че всяка точка силно "изтегля" границата към себе си.
Визуална демонстрация
Голямо 𝐶 и 𝛾: Моделът ще създаде сложна граница, която почти перфектно разделя точките, но е податлива на шум.
Малко 𝐶 и 𝛾: Моделът ще има по-проста граница, но може да пропусне по-сложни зависимости.


4.K-Nearest Neighbors (KNN)
Класифицира обекти въз основа на техните най-близки съседи.
Как да изберем броя на съседите (k)?
Малко 𝑘:
Моделът става чувствителен към шум.
Може да доведе до overfitting, защото взема предвид само най-близките точки.
Пример: Ако k=1, алгоритъмът просто копира най-близкия съсед, дори и той да е шум.
Голямо 𝑘:
Увеличава гладкостта на решението.
Може да доведе до underfitting, защото включва прекалено много точки, които може да не са свързани с локалната структура.
Как да изберем 𝑘:
Пробваме различни стойности чрез кръстосана проверка (cross-validation).
Най-често използвани стойности на 𝑘: малки нечетни числа (k=3,5,7), за да се избегнат равенства при класификация.
2. Как влияе мащабирането на данните върху KNN?
Защо е важно?
KNN използва евклидово разстояние (или друго метрика) за измерване на близостта между точки.
Ако характеристиките имат различни мащаби (например: една е в метри, друга в милиони долари), тези с по-големи стойности доминират разстоянието.
Решение:
Нормализация:
Преобразуване на характеристиките, така че те да са между 0 и 1.
Формула: (x−min(x))/(max(x)−min(x)).
Стандартизация:
Преобразуване на характеристиките с нулева средна стойност и единична стандартна девиация.
Формула: (x−μ)/σ, където 𝜇 е средната стойност, а 𝜎 е стандартното отклонение.
Пример:
Ако не мащабираме данните, характеристика като "години" може да бъде игнорирана спрямо "заплата в хиляди".

Слабости на KNN
Бавен за големи набори от данни:
Изисква изчисляване на разстояния за всяка точка при предсказване.
Решение: KD-Tree или Ball-Tree за оптимизация.
Чувствителен към шум и аномалии:
Пример: Аномална точка може да промени класификацията за съседите си.
Работи с всички характеристики:
Не взема предвид кои характеристики са релевантни за задачата.


5.Наивен Байес (Naive Bayes)
Наивен Байес е прост и ефективен алгоритъм за класификация, който се основава на Теоремата на Байес. Той приема, че всички характеристики (features) са независими една от друга, което често не е вярно на практика, но работи добре в много случаи.

Как работи Наивен Байес:
Трениране:
- Изчислява се вероятността на всяка категория (клас) P(C).
- Изчислява се вероятността на всяка характеристика, дадена категорията P(x∣C).

Класификация:
- За нови данни се изчислява апостериорната вероятност за всеки клас:

P(C∣X)∝P(C)⋅P(x 1∣C)⋅P(x 2∣C)⋅…⋅P(x n∣C)
Избира се класът с най-висока вероятност.


#### Ансамблови методи

1.Gradient Boosting (например XGBoost, LightGBM)
Подходящи за състезания по данни (като Kaggle).

Каква е разликата между boosting и bagging?
Boosting: Построява модели последователно, като всеки нов модел коригира грешките на предишния.
Bagging: Построява модели успоредно и усреднява резултатите.

Какво е значението на learning rate?
Контролира стъпките на оптимизацията. Малък learning rate = по-точна, но по-бавна оптимизация.


2.AdaBoost
Алгоритъм за подобряване на производителността на слаби ученици (weak learners).

Как работи теглото на примерите?
Идея: AdaBoost дава различно тегло на всяко наблюдение (пример) от данните.
Процес:
В началото всички примери получават еднакво тегло.
След всяка итерация (стъпка на обучение на weak learner):
Примерите, които са класифицирани грешно, получават по-високо тегло.
Примерите, които са класифицирани правилно, получават по-ниско тегло.
Алгоритъмът фокусира следващия weak learner върху трудните примери.
Цел: Подобряване на точността чрез коригиране на грешките.
2. Приложими случаи за AdaBoost
Силни страни:
Работи добре с таблични данни (structured data).
Подходящ за бинарна и мултикласова класификация.
Може да се използва за регресия (със съответна модификация).
Особено полезен, когато имаме много слаб обучаващ алгоритъм (например Decision Stump – decision tree с една стъпка).
Примери за реални случаи:
Кредитен риск (оценка на вероятността за неизпълнение).
Откриване на измами (fraud detection).
Класификация на клиенти за маркетинг кампании.
3. Как се избира weak learner?
Най-често се използват Decision Stumps:
Decision tree с дълбочина 1.
Лесен за обучение и бърз за изчисление.
Weak learners трябва да бъдат малко по-добри от случайното предположение, за да може AdaBoost да подобри производителността.
4. Как работи AdaBoost в стъпки?
Инициализация:
Всички примери започват с равни тегла.
Обучение на weak learner:
Weak learner се обучава върху данните, вземайки предвид текущите тегла.
Изчисляване на грешката:
Изчислява се грешката 𝜀: процент на неправилно класифицираните примери.
Актуализация на теглата:
Примерите, класифицирани грешно, получават по-високо тегло.
Комбиниране на weak learners:
Всеки weak learner получава тежест 𝛼, базирана на точността му.
Финалната хипотеза е вземане на мнозинство от всички weak learners.
5. Какви са слабостите на AdaBoost?
Чувствителен към шум в данните:
Примери с неправилни етикети могат да получат много високо тегло, което води до overfitting.
Изисква качествени данни и предобработка.
6. Какви метрики да използваме за оценка на AdaBoost?
Когато целта е класификация:
Accuracy, Precision, Recall, F1-Score.
ROC-AUC за оценка на способността за разделяне.
Когато целта е регресия:
Mean Absolute Error (MAE).
Mean Squared Error (MSE).


Алгоритми за Unsupervised Learning (без надзор)

#### Клъстеризация

1.K-Means
Популярен за групиране на данни в k клъстера.

Kак се избира броят на клъстерите (k)?
С помощта на метода "лактика" (elbow method) или силуетни коефициенти.

Какво е инерция (inertia)?
Мярка за разстоянието между точките в един клъстер и центъра на клъстера. По-малка инерция = по-компактни клъстери.

Защо изборът на начален центроид е важен?
Лош избор може да доведе до локални минимуми и неправилни клъстери.

Метод на "лактика" (Elbow Method)
Какво представлява?
Методът на "лактика" визуализира как стойността на вътрешноклъстерната сума от квадрати на грешките (Within-Cluster Sum of Squares, WCSS) намалява с увеличаването на броя на клъстерите 𝑘.
WCSS измерва сумата от квадратите на разстоянията между всяка точка и центъра на клъстера, към който тя принадлежи.
Намаляването на WCSS става по-бавно след определена стойност на 𝑘, което формира "лакът" на графиката.

2.Hierarchical Clustering (Йерархична клъстеризация)
Използва дендрограми за групиране на данни.
Може да те питат за: разликата между агломеративна и дивизивна клъстеризация.

3.DBSCAN (Density-Based Spatial Clustering)
Групира обекти въз основа на плътност.
cDBSCAN (Density-Based Spatial Clustering of Applications with Noise) е алгоритъм за кластеризация, който групира обекти въз основа на плътността на данните. Това означава, че той класифицира точки в групи (кластери), като взема предвид тяхната плътност в пространството. Основните параметри на DBSCAN са:
eps (epsilon): Това е радиусът на околната среда около дадена точка. Той определя разстоянието, в рамките на което обекти се считат за "съседни". Параметърът eps трябва да бъде настроен внимателно в зависимост от разпределението на данните.

min_samples: Това е минималният брой точки, които трябва да бъдат в околността на даден обект (определен от eps), за да бъде този обект считан за център на кластер. Тези точки трябва да образуват "плътна група", за да бъде обектът класифициран като ядро на кластер.

Как работят параметрите eps и min_samples:
eps (epsilon): Това е максималното разстояние между две точки, за да бъдат те част от един и същи кластер. Ако разстоянието между две точки е по-голямо от eps, те ще бъдат разглеждани като различни групи.
min_samples: Това е минималният брой точки, които трябва да има в околността на точка (в радиуса eps), за да бъде тя част от кластер. Тези точки формират "плътна зона", и ако тя не съдържа достатъчно точки, то точката ще бъде считана за "шум" (outlier) и няма да бъде част от кластер.

Какви са предимствата на DBSCAN пред K-Means:
Не изисква предварително задаване на броя на клъстерите: Докато в K-Means трябва да се зададе предварително броя на клъстерите, в DBSCAN не е нужно да се задава броя на клъстерите, тъй като той ги определя на базата на плътността на данните.
По-добра работа със сложни форми на кластери: DBSCAN може да открива клъстери с произволни форми, докато K-Means работи по-добре за сферични или правилно разположени клъстери.
Устойчивост на шум: DBSCAN може да открива и пренебрегва "шумни" точки (outliers), които не принадлежат към никакъв кластер, докато K-Means е по-чувствителен към тях и може да ги класифицира в неправилни клъстери.

Какви са основните недостатъци на DBSCAN:
Чувствителност към избор на параметрите eps и min_samples: Ако тези параметри не са избрани правилно, DBSCAN може да класифицира всички точки като шум или да не открие правилните клъстери.
Не работи добре с данни с различни плътности: Ако клъстерите имат много различна плътност, DBSCAN може да има проблеми при правилното им откриване.
Не може да се използва за твърде високи измерения: DBSCAN има проблеми в работата си с данни с много висока размерност, поради евристичната природа на алгоритъма за намиране на близки точки.


#### Намаляване на размерността

1.Principal Component Analysis (PCA)
Намалява размерността на данните, запазвайки възможно най-много вариация.

Как се избират компонентите?
Използвайки обяснената вариация (explained variance), избираме достатъчно компоненти, за да обясним >= 95% от общата вариация.

Каква е ролята на ковария матрицата?
Използва се за идентифициране на взаимовръзките между характеристиките и за изчисляване на основните компоненти.


2.t-SNE
Използва се за визуализация на високомерни данни в 2D/3D пространства.

t-SNE (t-Distributed Stochastic Neighbor Embedding) е алгоритъм за намаляване на размерността, който се използва основно за визуализация на високомерни данни в 2D или 3D пространства. Той е особено полезен, когато се работи с големи и сложни набори от данни и се иска да се запазят локалните отношения между данните в по-ниска размерност (като 2D или 3D).

Кога е подходящ t-SNE?
Визуализация на високомерни данни: t-SNE е подходящ, когато имате многомерни данни (например, характеристики от изображения, текстови данни или други големи набори от характеристики) и искате да ги визуализирате в 2D или 3D пространство, за да идентифицирате структури като клъстери или аномалии.
Откриване на клъстери и групи: t-SNE е много полезен за визуализиране на сложни структури, когато имате клъстери от данни, които искате да откриете или проучите в по-ниска размерност. Той може да помогне да видите как данните са разпределени и дали има някакви логически групи.
Анализ на сходства и разлики: t-SNE запазва локалните зависимости между точките в данните, което означава, че точки, които са близки в оригиналното пространство, ще бъдат близки и в намаленото пространство. Това го прави полезен за откриване на малки, но значими връзки в данните.

Какво е perplexity и как влияе на t-SNE?
Perplexity е хиперпараметър на t-SNE, който контролира баланса между локалните и глобалните структури в данните. Той е свързан с броя на съседите, които се вземат под внимание, когато се формира вероятностното разпределение за всяка точка.

Какво е Perplexity:
Perplexity може да се разглежда като мярка за неяснотата или сложността на разпределението на съседите за всяка точка. По принцип, стойности на perplexity между 5 и 50 се препоръчват, като по-ниски стойности означават, че се фокусирате върху локалната структура, докато по-високи стойности запазват по-голяма глобална информация.
Много високи стойности на perplexity могат да доведат до прекалено опростени резултати, които не отразяват добре истинската структура на данните. От друга страна, много малки стойности на perplexity могат да доведат до прекомерно фокусиране върху малки локални структури и да изгубите глобалната контекстуализация на данните.

Какви са ограниченията на t-SNE?
Трудно работи с много големи набори от данни: t-SNE е алгоритъм, който може да бъде много бавен за големи набори от данни. Въпреки че има оптимизации, използването на t-SNE за огромни набори от данни може да бъде трудно поради изискванията за памет и време за изчисление.
Няма глобална структура: t-SNE се фокусира върху запазването на локалните структури в данните, което може да доведе до това, че глобалната структура на данните (като разстояния между клъстери) не се запазва добре.
Трудно е да се тълкуват резултатите: Визуализациите, получени чрез t-SNE, са чудесни за откриване на интересни структури, но интерпретацията на тези структури не винаги е ясна. Това може да затрудни разбирането на какво точно показват данните.

Предимства на t-SNE:
Запазва локални зависимости: t-SNE е много ефективен за запазване на близостта между точки в оригиналното пространство и помага да се визуализират клъстери и други локални структури.
Полезен за визуализация на комплексни данни: t-SNE е много добър за визуализиране на данни с висока размерност, като генерира 2D или 3D проекции, които са лесни за разбиране и интерпретиране визуално.

В заключение:
t-SNE е мощен инструмент за визуализация на високомерни данни, който помага да се открият важни структури и зависимости. Въпреки това, изисква внимателно настройване на параметрите, като perplexity, за да се получат оптимални и интерпретируеми резултати.



Алгоритми за Reinforcement Learning (RL)

1.Q-Learning
Алгоритъм за обучение на агент чрез награди и наказания.

2.Deep Q-Learning (DQN)
Разширение на Q-Learning, използващо невронни мрежи.


Deep Learning алгоритми

1.Невронни мрежи (Neural Networks)
Използват се за комплексни задачи като изображения и текст.
Може да те питат за: основни слоеве (Dense, Convolutional, Recurrent), оптимизатори (Adam, SGD), функция на грешка.

2.Convolutional Neural Networks (CNNs)
Подходящи за изображения.
Може да те питат за: какво са конволюции, как работят филтрите.

3.Recurrent Neural Networks (RNNs)
Подходящи за последователности, като текст или времеви серии.


##########################################################################################

Метрики за оценка на моделите

Класификация: Accuracy, Precision, Recall, F1-Score, ROC AUC.
Регресия: MSE, RMSE, MAE, R².

##########################################################################################

Разликата между Decision Trees (решаващи дървета) и Random Forest (случайна гора) се основава на тяхната структура, метод на обучение и поведение при различни задачи. Ето основните разлики:

1. Основна структура
Decision Tree:
Съдържа само едно дърво, което разделя данните на базата на правила.
Работи като йерархична структура, в която всеки възел разделя данните според определена характеристика.
Random Forest:
Използва множество решаващи дървета, които работят заедно като ансамблов модел.
Крайният резултат е комбинация от предвижданията на всички дървета (средно аритметично за регресия или гласуване за класификация).
2. Точност и стабилност
Decision Tree:
Може лесно да се адаптира към обучителните данни, но това го прави податливо на overfitting.
Чувствително е към малки промени в данните, което може да доведе до различни структури на дървото.
Random Forest:
Намалява проблема с overfitting, защото обобщава резултатите от много дървета.
По-стабилен и по-точен при работа с шумни и сложни данни.
3. Случайност (Randomness)
Decision Tree:
Използва всички налични данни и характеристики, за да разделя възлите.
Random Forest:
Добавя случайност чрез:
Случайни поднабори от данните (Bagging).
Случайни поднабори от характеристиките при разделяне на възли.
4. Скорост
Decision Tree:
По-бързо за обучение и предвиждане, тъй като използва само едно дърво.
Random Forest:
По-бавно за обучение и предвиждане, защото тренира множество дървета и комбинира резултатите.
5. Интерпретация
Decision Tree:
Лесно интерпретируемо – можеш да проследиш решенията, взети от модела, за конкретно предвиждане.
Random Forest:
Трудно интерпретируемо, защото комбинира много дървета, всяко със свои собствени правила.
6. Приложение
Decision Tree:
Добър избор за задачи с малки или средно големи данни, където точността не е критична.
Random Forest:
Подходящ за сложни задачи, където точността и стабилността са важни, като кредитен риск, прогнозиране на отлив (churn prediction) и откриване на измами.
7. Изчисляване на важността на характеристиките
Decision Tree:
Показва кои характеристики са най-важни в конкретното дърво.
Random Forest:
Изчислява общата важност на характеристиките, базирана на приноса им към всички дървета.
Пример
Decision Tree:
Моделът може да направи грешни предвиждания, ако структурата на данните е твърде сложна или шумна.

Random Forest:
Комбинира множество дървета, като заглушава грешките на индивидуалните дървета и предоставя по-точни предвиждания.

######################################################################################################
Как да изберем метриката за разстояние?

Евклидово разстояние: Най-често използвано.
Подходящо за данни в еднаква мащабна скала.

Манхатън разстояние:
Използва сума от абсолютните разлики между координатите.
Подходящо, ако данните са разпръснати по осите.

Косинусова подобност:
Подходяща за текстови данни или вектори с голяма размерност.

Махааланобис разстояние:
Взема предвид корелацията между характеристиките.
Подходящо за данни с различна разпределеност.

#####################################################################################################

Ансамбловите методи (ensemble methods) са техники в машинното обучение, при които се комбинират множество модели, за да се постигне по-добра обща производителност, отколкото всеки от отделните модели би постигнал самостоятелно. Основната идея е, че комбинирайки няколко по-слаби модела (weak learners), можем да създадем по-силен и по-устойчив модел (strong learner).

Как работят ансамбловите методи?
Ансамбловите методи разчитат на следното:
Диверсификация: Използването на различни модели или различни версии на един и същи модел води до по-малко пристрастия и по-голяма устойчивост.
Комбинация: Резултатите от всички модели се обединяват чрез агрегиране (например осредняване или гласуване).

Основни видове ансамблови методи

Bagging (Bootstrap Aggregating):
Основна идея: Намалява вариацията в модела чрез комбиниране на резултатите от няколко независими модела, обучени върху различни подмножества от данните.
Пример: Random Forest.
Random Forest обучава множество дървета за вземане на решения (decision trees), като всеки от тях работи с различна случайна подмножество от данните и характеристиките.
Намалява overfitting, защото обединява резултатите чрез усредняване (за регресия) или гласуване (за класификация).

Boosting:
Основна идея: Намалява грешките в модела чрез последователно обучение на модели, където всеки следващ модел коригира грешките на предходния.
Примери: AdaBoost, Gradient Boosting, XGBoost.
Тези алгоритми поставят по-голяма тежест на грешно класифицираните или прогнозирани точки, за да подобрят точността.

Stacking (Stacked Generalization):
Основна идея: Комбинира различни модели чрез метамодел, който се обучава да предсказва крайните резултати въз основа на изходите на базовите модели.
Пример: Използване на Logistic Regression или Neural Network като метамодел, който обединява резултатите от Random Forest, SVM, и други.

Защо ансамблите намаляват overfitting?
Диверсификация на грешките: Различните модели правят различни грешки, които се неутрализират при обединяването.
Намаляване на вариацията: Обединяването на резултатите от няколко модела води до по-стабилни и надеждни предсказания.
Случайност: В Random Forest, например, случайното избиране на данни и характеристики при обучение на дърветата намалява зависимостта от конкретен набор данни.

########################################################################################################################

Вариация в контекста на статистика и машинно обучение е мярка за разпръснатостта или разнообразието на данните. Тя показва колко различават стойностите на дадена променлива от средната стойност (средната аритметична стойност).

Пример:
Ако имаме набор от данни за височините на група хора и изчислим средната височина, то вариацията ще показва колко широко разположени са тези височини около средната стойност. Ако повечето хора са с височина близо до средната, вариацията ще бъде малка. Ако има голяма разлика между височините, вариацията ще бъде голяма.

В машинно обучение и PCA:
При Principal Component Analysis (PCA), основната цел е да се намали размерността на данните, като се запази възможно най-много вариация от оригиналните данни. Тоест, новите компоненти трябва да обясняват възможно най-голямата част от информацията (вариацията), която е налице в оригиналните данни.

Вариация тук е свързана с това колко разпръснати са данните в различни посоки и как да се използват тези посоки (нови компоненти), за да се обясни максимално количество от информацията в данните.
В контекста на PCA:
PCA избира компонентите, които запазват най-голямата вариация в данните и намалява измеренията, като се опитва да запази основните характеристики на данните. Това обикновено се прави чрез намиране на главни компоненти, които са линейни комбинации на оригиналните признаци, и така се опростява структурата на данните.

#########################################################################################################################

Теорията на грешките

Bias (Систематична грешка)
Bias представлява грешка, която възниква поради прекалено опростяване на модела. Той не успява да улови сложността на данните и води до ниска производителност както при тренировъчния, така и при тестовия набор от данни.

Характеристики на висок bias:
Моделът е прекалено прост (underfitting).
Невъзможност за улавяне на сложни зависимости.
Грешките са високи както върху тренировъчните, така и върху тестовите данни.
Пример за висок bias:
Използването на линейна регресия за данни с нелинейна зависимост.

Variance (Разнообразие)
Variance представлява грешка, която възниква поради прекалено адаптиране към тренировъчните данни. Моделът е прекалено сложен и се "нагласява" към шума в данните, което води до слаба обобщаваща способност върху нови (тестови) данни.

Характеристики на висок variance:
Моделът е прекалено сложен (overfitting).
Отлична производителност върху тренировъчните данни.
Лоша производителност върху тестовите данни.
Пример за висок variance:
Използването на полиномиална регресия от 10-ти ред за данни с линейна зависимост.


