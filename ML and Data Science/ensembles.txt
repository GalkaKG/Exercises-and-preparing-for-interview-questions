Ансамбли в машинното обучение са методи, които комбинират множество отделни модели, за да подобрят цялостната производителност и точност на прогнозите. 
Основната идея е, че като се комбинират резултатите от различни модели, могат да се намалят грешките, които всеки от тях би направил поотделно.

Видове ансамблови методи"

**Bagging (Bootstrap Aggregating):

Основна идея: Намалява вариацията на модела, като комбинира прогнози от множество версии на същия модел, обучени върху различни случайни подмножества от данните.
Пример: Random Forest
Създава много дървета на решение, като всяко дърво е обучено върху случайна подмножество от данните. 
Крайният резултат е средното (за регресия) или гласуването (за класификация) от всички дървета.

**Boosting:

Основна идея: Намалява bias-а, като създава последователност от модели, като всеки нов модел се фокусира върху грешките, направени от предишния.
Примери:
AdaBoost: Добавя тежест на грешно класифицираните примери и обучава нови модели, за да коригират тези грешки.
Gradient Boosting: Оптимизира грешките чрез минимизиране на остатъците в последователни модели.
XGBoost и LightGBM: Подобрени версии на Gradient Boosting, които са по-бързи и по-ефективни.

**Stacking:

Основна идея: Комбинира резултатите от няколко различни модела чрез друг модел (мета-модел), който се обучава върху прогнозите на базовите модели.
Пример: Използване на линейна регресия или невронна мрежа като мета-модел, за да комбинира прогнози от Random Forest, Gradient Boosting и Support Vector Machines.

**Voting:

Основна идея: Комбинира прогнози от няколко модела чрез гласуване.
Видове:
Мажоритарно гласуване: За класификация – избира класа, който е предсказан най-често от моделите.
Средно гласуване: За регресия – използва средната стойност на прогнозите.

Защо ансамбли са полезни?
Намаляват грешките:

Bias: Boosting помага за справяне с недостатъчно адаптирани (underfitting) модели.
Variance: Bagging помага за справяне с прекалено адаптирани (overfitting) модели.

Подобряват стабилността:
Ансамблите намаляват влиянието на случайността, която може да повлияе на отделен модел.

По-добра обобщаваща способност:
Моделите в ансамблите често се допълват един друг и улавят различни аспекти на данните.

Кога да използваме ансамбли?
Когато имаме сложен проблем и отделните модели не дават достатъчно добра точност.
Когато се опитваме да намалим overfitting или underfitting.
При състезания, като тези на Kaggle, ансамбловите методи често водят до топ резултати.


Пример с Random Forest (Bagging) в Python:

from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Зареждане на данни
data = load_iris()
X, y = data.data, data.target

# Разделяне на данните
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Създаване на Random Forest
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Оценка на точността
predictions = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, predictions))


